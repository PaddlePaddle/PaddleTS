batch_size: 32
seq_len: 96
predict_len: 720
do_eval: True
epoch: 10


dataset: 
  name: ETTh2
  split:
    train: [0, 8640]  # 12 * 30 * 24
    val: [8640, 11520]  # [12 * 30 * 24 - seq_len, 12 * 30 * 24 + 4 * 30 * 24]
    test: [11520, 14400] # [12 * 30 * 24 + 4 * 30 * 24 - seq_len, 12 * 30 * 24 + 8 * 30 * 24]


model: 
  name: Nonstationary_Transformer
  model_cfg:
    c_in: 7
    factor: 1
    p_hidden_dims: [256, 256]
    optimizer_params:
      learning_rate: 0.0001
      gamma: 0.5
    patience: 5
    #pretrain: non-station_torch.pdparams

test:
  stride: 1

  



