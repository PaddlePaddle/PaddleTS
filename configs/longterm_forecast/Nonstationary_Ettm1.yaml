batch_size: 32
seq_len: 96
predict_len: 720
do_eval: True
epoch: 10


dataset: 
  name: ETTm1
  split:
    train: [0, 34560]  # 12 * 30 * 24
    val: [34560, 46080]  # [12 * 30 * 24 - seq_len, 12 * 30 * 24 + 4 * 30 * 24]
    test: [46080, 57600] # [12 * 30 * 24 + 4 * 30 * 24 - seq_len, 12 * 30 * 24 + 8 * 30 * 24]


model: 
  name: Nonstationary_Transformer
  model_cfg:
    c_in: 7
    factor: 1
    p_hidden_dims: [256, 256]
    optimizer_params:
      learning_rate: 0.0001
      gamma: 0.5
    patience: 5
    #pretrain: non-station_torch.pdparams

test:
  stride: 1

  



